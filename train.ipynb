{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62da0e27",
   "metadata": {},
   "source": [
    "# Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deba7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import pickle \n",
    "\n",
    "# Import ML library and its modules\n",
    "import sklearn\n",
    " \n",
    "# Library for Dealing with imbalanced datasets\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## Import train_test_split, cross_val_score, KFold - Validation and GridSearchCV  \n",
    "from sklearn.model_selection import  train_test_split, cross_val_score, GridSearchCV, KFold \n",
    "\n",
    "# Import Classifiers - Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "# Data Engineering\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import Metrics - Performance Evaluation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c357f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433db084",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4f7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "feature_importance_threshold = 0.005\n",
    "test_size = 0.2\n",
    "random_state = 100\n",
    "path=\"./\"\n",
    "file_name=\"sampled_webpages_classification_train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f6de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, file_name):\n",
    "    df = pd.read_csv(path + file_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51b4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['label']=np.where(df['label']=='bad', 1, 0) # bad=1 and good=0\n",
    "\n",
    "##  Useful if there is any data conversion that needs to be done by numerical or categorical features\n",
    "#   num_cols = (train.select_dtypes(exclude=\"object\").columns.values).tolist()\n",
    "#   cat_cols = train.select_dtypes(include=\"object\").columns.values.tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4743ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_data(df):\n",
    "#     # Numerical \n",
    "#     ## Univariate\n",
    "#     ## Bivariate\n",
    "#     ## Multivariate\n",
    "    \n",
    "#     # Categorical \n",
    "#     ## Univariate\n",
    "#     ## Bivariate\n",
    "#     ## Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4baae548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extractor(df):\n",
    "    \n",
    "    # Extract URL features \n",
    "    df[\"asperand_symbol\"] = np.where(df['url'].str.contains(\"@\"), 1, 0) # Existence of Asperand - @ symbol\n",
    "    df[\"redirection_symbol\"] = np.where(df['url'].str.removeprefix(\"http://\").str.removeprefix(\"https://\").str.contains(\"//\"),\n",
    "                                        1, 0) # Redirection Symbol // symbol\n",
    "    \n",
    "    \n",
    "    df[\"hyphen_symbol\"] = np.where(df[\"url\"].apply(lambda x: urlparse(x).netloc).str.contains(\"-\"),\n",
    "                                   1, 0) # Hyphen(-) Symbol\n",
    "    \n",
    "    df[\"multilevel_subdomains\"] = np.where(df[\"url\"].apply(lambda x: urlparse(x).netloc).str.count(\"\\.\") > 3, 1, 0)\n",
    "    \n",
    "    df['content_len'] = df[\"content\"].str.len()\n",
    "    \n",
    "    # Extract content features \n",
    "    df[\"iframe\"] = np.where(df[\"content\"].str.findall(\"<iframe>\"), 1, 0) # Presence of iframe\n",
    "    df[\"no_of_iframes\"]= df[\"content\"].str.findall(\"<iframe>\").apply(lambda x: len(x)) # No. of iframes \n",
    "    df[\"no_of_find_fn\"] = df[\"content\"].str.findall(\"find\\(\\)\").apply(lambda x: len(x)) # No. of find() fns used\n",
    "    df[\"no_of_eval_fn\"] = df[\"content\"].str.findall(\"eval\\(\\)\").apply(lambda x: len(x))  # No. of eval() fns used\n",
    "    \n",
    "    df[\"content\"] = df[\"content\"].str.lower()\n",
    "    df[\"content\"] = df[\"content\"].str.replace('\\d+',\" \", regex=True) # Replacing numbers with spaces   \n",
    "    df['content_len'] = df['content'].str.len() \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223ea640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def content_tf_idf_vectorizer(df, max_features=150, stop_words='english'): \n",
    "        \n",
    "#     vectorizer = TfidfVectorizer(max_features=max_features, stop_words=\"english\") # Defining vectorizer\n",
    "#     vectorized_content = vectorizer.fit_transform(train[\"content\"] )\n",
    "    \n",
    "#     vectorized_content_df = pd.DataFrame(data=vectorized_content.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "#     df = df.join(vectorized_content_df, how=\"left\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2646f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_ip_content_features(df): # As they are unique and do not help in our modeling\n",
    "    df.drop(['url', 'ip_add', 'content'], axis=1, inplace =True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25dd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_dictvectorizer(df):    \n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    \n",
    "    df_dict = df.to_dict(orient='records')\n",
    "    df_dv = dv.fit_transform(df_dict)\n",
    "    \n",
    "    df_dv_cols = list(dv.get_feature_names_out())\n",
    "    \n",
    "    df = pd.DataFrame(data=df_dv, columns=df_dv_cols)\n",
    "    \n",
    "    return dv, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a5e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(df):\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    return  x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea23b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the dataset samples to get equal samples for both classes to train\n",
    "def train_balance_resampling(x_train, y_train):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    SMOTE = SMOTE()\n",
    "    x_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(x_train, y_train) # fit and resample to get equal samples for both classes\n",
    "    \n",
    "    return  x_train_SMOTE, y_train_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6208c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based Feature selection using RandomForestClassifier to select feature to consider for training\n",
    "def feature_selection(x_train, y_train):\n",
    "    \n",
    "    rndf = RandomForestClassifier(n_estimators=150)\n",
    "    rndf.fit(x_train, y_train)\n",
    "    \n",
    "    importance = pd.DataFrame.from_dict({'cols':x_train.columns, 'importance': rndf.feature_importances_})\n",
    "    importance = importance.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    imp_cols = list(importance[importance.importance >= feature_importance_threshold].cols.values)\n",
    "    \n",
    "    return x_train[imp_cols], y_train, imp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040e278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(path, file_name):\n",
    "    \n",
    "    df = load_data(path, file_name)\n",
    "    df = clean_data(df)\n",
    "    df = data_extractor(df)\n",
    "    df = remove_url_ip_content_features(df)\n",
    "    dv, df = df_dictvectorizer(df)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_validation_split(df)\n",
    "    \n",
    "    x_train_SMOTE, y_train_SMOTE = train_balance_resampling(x_train, y_train)\n",
    "    \n",
    "    x_train_SMOTE, y_train_SMOTE, imp_cols = feature_selection(x_train_SMOTE, y_train_SMOTE)\n",
    "   \n",
    "    # Hyperparameter tuning with GridSearchCV i.e. includes K-Fold crossvalidation\n",
    "    param_grid = {'criterion':['gini','entropy','cross_entropy'],\n",
    "                  'max_depth':[2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                  'random_state':[100],\n",
    "                  'n_estimators':[200,400,600],\n",
    "                  'n_jobs':[-1], \n",
    "                  'random_state':[100],\n",
    "                  'verbose': [0]\n",
    "                 }\n",
    "\n",
    "    RF_grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv = 5, scoring='balanced_accuracy', verbose=1)\n",
    "\n",
    "    RF_grid.fit(x_train_SMOTE, y_train_SMOTE)\n",
    "    \n",
    "    model = RF_grid.best_estimator_\n",
    "     \n",
    "    return imp_cols, dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e7b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d098f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "# Assigning the selected_features, dictvectorizer and the trained model objects to a variable\n",
    "selected_features, dv, model = training(path, file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90930a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bc07c0",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50a1c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model.bin\"\n",
    "\n",
    "with open(model_file,'wb') as f_out:\n",
    "    pickle.dump((selected_features, dv, model), f_out) # Saving the selected_features, dictvectorizer and the trained model objects to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a32aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved to model.bin\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model is saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41550e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
